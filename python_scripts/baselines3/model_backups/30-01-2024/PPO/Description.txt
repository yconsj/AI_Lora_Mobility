Used for PPO Vs DQN evaluation.
Properties:
    envs = 16
    env = make_vec_env(make_skipped_env, n_envs=envs, vec_env_cls=SubprocVecEnv)
    gamma = 0.85  # base: 0.85
    ent_coef = 0.005  # base: 0.005
    learning_rate = 1e-4  # base: 6e-5

    n_blocks = 3  # # base: 2

    stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=50, min_evals=100, verbose=1)
    eval_callback = EvalCallback(env, eval_freq=4000, callback_after_eval=stop_train_callback,
                                 verbose=1, best_model_save_path="stable-model-2d-best")

    policy_kwargs = dict(
        #features_extractor_class=CustomPolicyNetwork,
        #features_extractor_kwargs=dict(features_dim=64, num_blocks=n_blocks),
        #activation_fn=nn.ReLU,
        share_features_extractor=True,
        net_arch=[64, 64, 64]
    )
	model = PPO("MlpPolicy", env, device="cpu", learning_rate=learning_rate, gamma=gamma, ent_coef=ent_coef,
			batch_size=64,  # base: 64
			clip_range=0.15,  # base: 0.15
			n_steps=4096,  # one episode is roughly 4000 steps, when using time_skip=10 # base: 4096
			n_epochs=10,  # base: 10
			policy_kwargs=policy_kwargs,
			tensorboard_log="./tensorboard/",
			)
	model = model.learn(8_000_000, callback=[eval_callback, TensorboardCallback()],
                        tb_log_name=tb_log_name)
						